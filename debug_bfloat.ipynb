{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import lightning\n",
    "import lightning.pytorch\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "\n",
    "from quiet_star.config import Config, ModelConfig\n",
    "from quiet_star.constants import END_THOUGHT_TOKEN, START_THOUGHT_TOKEN\n",
    "from quiet_star.torch.utils import assert_shape, expand_dims, torch_dtype\n",
    "from quiet_star.torch.pretrained import PretrainedThoughtModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_inputs(\n",
    "    model: lightning.LightningModule, config: Config, text: str, max_thought_length: int\n",
    ") -> tuple[list[int], list[list[int]]]:\n",
    "    x = model.tokenizer(\n",
    "        text,\n",
    "        padding=\"do_not_pad\",\n",
    "        truncation=True,\n",
    "        max_length=config.model.max_length - config.thought_length - 2,\n",
    "        return_tensors=\"np\",\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"][0].tolist()\n",
    "    start_thought_token = model.tokenizer(\n",
    "        START_THOUGHT_TOKEN, return_tensors=\"np\", return_attention_mask=False\n",
    "    )[\"input_ids\"][0, 0].tolist()\n",
    "    thought_tokens = [[start_thought_token] for _ in range(len(x))]\n",
    "    if max_thought_length > 0:\n",
    "        next_tokens = [\n",
    "            [\n",
    "                random.randrange(0, len(model.tokenizer))\n",
    "                for _ in range(max_thought_length)\n",
    "            ]\n",
    "            for _ in range(len(x))\n",
    "        ]\n",
    "        thought_tokens = [thought_tokens[i] + next_tokens[i] for i in range(len(x))]\n",
    "\n",
    "    return x, thought_tokens\n",
    "\n",
    "def prepare_next_thought_token_input(\n",
    "    x: list[int],\n",
    "    thought_tokens: list[list[int]],\n",
    "    thought_length: int,\n",
    "    batch_size: int,\n",
    "    device: str | torch.device,\n",
    "    last_thought_token_only: bool,\n",
    ") -> torch.Tensor:\n",
    "    if not last_thought_token_only:\n",
    "        thoughts = [tokens[:thought_length] for tokens in thought_tokens]\n",
    "        x = torch.tensor(x, dtype=torch.int64, device=device)\n",
    "        x = torch.unsqueeze(x, dim=-1)\n",
    "        thoughts = torch.tensor(thoughts, dtype=torch.int64, device=device)\n",
    "        inputs = torch.concatenate([x, thoughts], dim=-1).tolist()\n",
    "    else:\n",
    "        thoughts = [[tokens[thought_length - 1]] for tokens in thought_tokens]\n",
    "        inputs = thoughts\n",
    "\n",
    "    return torch.tensor(\n",
    "        [inputs for _ in range(batch_size)], dtype=torch.int64, device=device\n",
    "    )  # add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/src/quiet-star/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 468.19M\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = Config(\n",
    "    batch_size=2,\n",
    "    thought_length=3,\n",
    "    model=ModelConfig(\n",
    "        attn_type=\"torch\",\n",
    "        device=device,\n",
    "        dropout_attn=0.0,\n",
    "        dropout_embed=0.0,\n",
    "        dtype=\"bfloat16\",\n",
    "        model_name=\"Qwen/Qwen1.5-0.5B-Chat\",\n",
    "        max_length=32,\n",
    "    ),\n",
    ")\n",
    "model = PretrainedThoughtModel(config).to(config.model.device)\n",
    "activation_cache = None\n",
    "x, thought_tokens = prepare_test_inputs(\n",
    "    model, config, \"This is a test.\", config.thought_length\n",
    ")\n",
    "t = 1\n",
    "i = 0\n",
    "# correct\n",
    "xi = torch.tensor(\n",
    "    [x[: i + 1] + thought_tokens[i][:t]],\n",
    "    dtype=torch.int64,\n",
    "    device=model.device,\n",
    ")\n",
    "# testing\n",
    "ax = prepare_next_thought_token_input(\n",
    "    x,\n",
    "    thought_tokens,\n",
    "    t,\n",
    "    config.batch_size,\n",
    "    model.device,\n",
    "    last_thought_token_only=(t > 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 5 2\n"
     ]
    }
   ],
   "source": [
    "# correct\n",
    "b, l = xi.shape\n",
    "# testing\n",
    "ab, al, ad = ax.shape\n",
    "if activation_cache is None:\n",
    "    activation_cache = [{} for _ in range(len(model.layers))]\n",
    "print(ab, al, ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "causal_mask1 = torch.triu(\n",
    "    torch.full((l, l), float(\"-inf\"), dtype=model._dtype, device=model.device),\n",
    "    diagonal=1,\n",
    ")\n",
    "causal_mask1 = causal_mask1.unsqueeze(0)\n",
    "# testing\n",
    "acausal_mask1 = torch.triu(\n",
    "    torch.full((al, al), float(\"-inf\"), dtype=model._dtype, device=model.device),\n",
    "    diagonal=1,\n",
    ")\n",
    "acausal_mask1 = expand_dims(acausal_mask1, (0, 1, 3))\n",
    "acausal_mask2 = torch.triu(\n",
    "    torch.full(\n",
    "        (t + 1, t + 1), float(\"-inf\"), dtype=model._dtype, device=model.device\n",
    "    ),\n",
    "    diagonal=1,\n",
    ")\n",
    "acausal_mask2 = expand_dims(acausal_mask2[t - ad + 1 :, 1:], (0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_ids: torch.Size([1, 2]) tensor([[0, 1]], device='cuda:0')\n",
      "aposition_ids: torch.Size([2, 5, 2]) tensor([[[0, 1],\n",
      "         [1, 2],\n",
      "         [2, 3],\n",
      "         [3, 4],\n",
      "         [4, 5]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [1, 2],\n",
      "         [2, 3],\n",
      "         [3, 4],\n",
      "         [4, 5]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# correct\n",
    "row = torch.arange(0, l, dtype=torch.int64, device=model.device)\n",
    "position_ids = row.reshape(1, l).tile((b, 1))\n",
    "xi2 = model.tok_emb(xi)\n",
    "# testing\n",
    "arows = torch.arange(0, ad, dtype=torch.int64, device=model.device).reshape(1, ad)\n",
    "arow_offsets = torch.arange(\n",
    "    t - ad + 1, al + t - ad + 1, dtype=torch.int64, device=model.device\n",
    ").reshape(al, 1)\n",
    "aposition_ids = (arows + arow_offsets).reshape((1, al, ad)).tile((ab, 1, 1))\n",
    "ax2 = model.tok_emb(ax)\n",
    "\n",
    "print(\"position_ids:\", position_ids.shape, position_ids)\n",
    "print(\"aposition_ids:\", aposition_ids.shape, aposition_ids)\n",
    "assert torch.allclose(xi2, ax2[:1, 0, :2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "layer = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "residual = xi2\n",
    "xi3 = layer.input_layernorm(xi2)\n",
    "# testing\n",
    "aresidual = ax2\n",
    "ax3 = model.bfloat_safe_apply(layer.input_layernorm, ax2)\n",
    "ax3p = layer.input_layernorm(ax2)\n",
    "assert torch.allclose(xi3, ax3[:1, 0, :2, :])\n",
    "assert torch.allclose(xi3, ax3p[:1, 0, :2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "q = (\n",
    "    layer.self_attn.q_proj(xi3)\n",
    "    .reshape(b, l, model.num_heads, -1)\n",
    "    .permute([0, 2, 1, 3])\n",
    ")\n",
    "k = (\n",
    "    layer.self_attn.k_proj(xi3)\n",
    "    .reshape(b, l, model.num_heads, -1)\n",
    "    .permute([0, 2, 1, 3])\n",
    ")\n",
    "v = (\n",
    "    layer.self_attn.v_proj(xi3)\n",
    "    .reshape(b, l, model.num_heads, -1)\n",
    "    .permute([0, 2, 1, 3])\n",
    ")\n",
    "# testing\n",
    "aq = (\n",
    "    model.bfloat_safe_apply(layer.self_attn.q_proj, ax3)\n",
    "    .reshape(ab, al, ad, model.num_heads, -1)\n",
    "    .permute([0, 3, 1, 2, 4])\n",
    ")\n",
    "ak1 = (\n",
    "    model.bfloat_safe_apply(layer.self_attn.k_proj, ax3[:, :, :1, :])\n",
    "    .reshape(ab, al, 1, model.num_heads, -1)\n",
    "    .permute([0, 3, 1, 2, 4])\n",
    ")\n",
    "ak2 = (\n",
    "    model.bfloat_safe_apply(layer.self_attn.k_proj, ax3[:, :, 1:, :])\n",
    "    .reshape(ab, al, ad - 1, model.num_heads, -1)\n",
    "    .permute([0, 3, 1, 2, 4])\n",
    ")\n",
    "av1 = (\n",
    "    model.bfloat_safe_apply(layer.self_attn.v_proj, ax3[:, :, :1, :])\n",
    "    .reshape(ab, al, 1, model.num_heads, -1)\n",
    "    .permute([0, 3, 2, 1, 4])\n",
    ")\n",
    "av2 = (\n",
    "    model.bfloat_safe_apply(layer.self_attn.v_proj, ax3[:, :, 1:, :])\n",
    "    .reshape(ab, al, ad - 1, model.num_heads, -1)\n",
    "    .permute([0, 3, 1, 2, 4])\n",
    ")\n",
    "activation_cache[i][\"k1\"] = ak1\n",
    "activation_cache[i][\"v1\"] = av1\n",
    "assert torch.allclose(q, aq[:1, :, 0, :2, :])\n",
    "assert torch.allclose(k[:, :, :1, :], ak1[:1, :, 0, :1, :]), f\"\\nk: {k[:, :, :1, :]}\\nak1:{ak1[:1, :, 0, :1, :]}\"\n",
    "assert torch.allclose(k[:, :, 1:, :], ak2[:1, :, 0, :1, :]), f\"\\nk: {k[:, :, 1:, :]}\\nak2:{ak2[:1, :, 0, :1, :]}\"\n",
    "assert torch.allclose(v[:, :, :1, :], av1[:1, :, 0, :1, :])\n",
    "assert torch.allclose(v[:, :, 1:, :], av2[:1, :, 0, :1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "cos, sin = layer.self_attn.rotary_emb(v, seq_len=l)\n",
    "qp = model.apply_rotary_pos_emb(q, cos, sin, position_ids)\n",
    "kp = model.apply_rotary_pos_emb(k, cos, sin, position_ids)\n",
    "# testing\n",
    "acos, asin = layer.self_attn.rotary_emb(av1, seq_len=al + t + 1)\n",
    "aqp = model.apply_rotary_pos_emb(aq, acos, asin, aposition_ids)\n",
    "ak1p = model.apply_rotary_pos_emb(ak1, acos, asin, aposition_ids[:, :, :1])\n",
    "if ad > 1:  # only apply to k2 if it is nonempty\n",
    "      ak2p = model.apply_rotary_pos_emb(ak2, acos, asin, aposition_ids[:, :, 1:])\n",
    "assert torch.allclose(qp, aqp[:1, :, 0, :2, :]), f\"\\nq: {qp}\\naq: {aqp[:1, :, 0, :2, :]}\"\n",
    "assert torch.allclose(kp[:, :, :1, :], ak1p[:1, :, 0, :1, :]), f\"\\nk: {kp[:, :, :1, :]}\\nak1:{ak1p[:1, :, 0, :1, :]}\"\n",
    "assert torch.allclose(kp[:, :, 1:, :], ak2p[:1, :, 0, :1, :]), f\"\\nk: {kp[:, :, 1:, :]}\\nak2:{ak2p[:1, :, 0, :1, :]}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "a = torch.nn.functional.softmax(\n",
    "    (torch.matmul(qp, kp.permute([0, 1, 3, 2])) + causal_mask1)\n",
    "    / math.sqrt(model.embed_dim / model.num_heads),\n",
    "    dim=-1,\n",
    ")\n",
    "# testing\n",
    "aa = torch.nn.functional.softmax(\n",
    "    torch.concatenate(\n",
    "        [\n",
    "            # attend to tokens in original string\n",
    "            # (B, H, L, D, E) @ (B, H, 1, E, L) => (B, H, L, D, L)\n",
    "            torch.matmul(aqp, ak1p.permute([0, 1, 3, 4, 2])) + acausal_mask1,\n",
    "            # attend to thought tokens generated so far\n",
    "            # (B, H, L, D, E) @ (B, H, L, E, T) => (B, H, L, D, T)\n",
    "            torch.matmul(aqp, ak2p.permute([0, 1, 2, 4, 3])) + acausal_mask2,\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "    / math.sqrt(model.embed_dim / model.num_heads),\n",
    "    dim=-1,\n",
    ")\n",
    "aa1 = aa[:, :, :, :, :al]\n",
    "aa2 = aa[:, :, :, :, al:]\n",
    "assert torch.allclose(a[:, :, :, :1], aa1[:1, :, 0, :, :1])\n",
    "assert torch.allclose(a[:, :, :, 1:], aa2[:1, :, 0, :, :]), f\"\\na:{a[:, :, :, 1:]}\\naa2:{aa2[:1, :, 0, :, :]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nattn_out: torch.Size([1, 16, 2, 64]),tensor([[[[ 0.0223,  0.0140,  0.0221,  ...,  0.0361, -0.0466,  0.0210],\n          [ 0.0123,  0.0040,  0.0118,  ...,  0.0154, -0.0209,  0.0109]],\n\n         [[-0.0042,  0.0049,  0.0140,  ..., -0.0007,  0.0082, -0.0055],\n          [-0.0033,  0.0045,  0.0096,  ..., -0.0007,  0.0047, -0.0022]],\n\n         [[-0.0107, -0.0145, -0.0123,  ..., -0.0063, -0.0124, -0.0059],\n          [-0.0064, -0.0114, -0.0081,  ..., -0.0054, -0.0089, -0.0035]],\n\n         ...,\n\n         [[-0.0317,  0.0034, -0.0103,  ...,  0.0058,  0.0129, -0.0070],\n          [-0.0280,  0.0036, -0.0079,  ...,  0.0047,  0.0114, -0.0060]],\n\n         [[ 0.0179,  0.0233,  0.0038,  ..., -0.0051, -0.0023,  0.0128],\n          [ 0.0164,  0.0205,  0.0033,  ..., -0.0039, -0.0020,  0.0116]],\n\n         [[-0.0223,  0.0110, -0.0292,  ..., -0.0175,  0.1069, -0.0026],\n          [-0.0146,  0.0084, -0.0177,  ..., -0.0134,  0.0718, -0.0036]]]],\n       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\naattn_out: torch.Size([1, 16, 2, 64]),tensor([[[[ 0.0223,  0.0140,  0.0221,  ...,  0.0361, -0.0466,  0.0210],\n          [ 0.0123,  0.0040,  0.0118,  ...,  0.0154, -0.0209,  0.0109]],\n\n         [[-0.0042,  0.0049,  0.0140,  ..., -0.0007,  0.0082, -0.0055],\n          [-0.0033,  0.0045,  0.0096,  ..., -0.0007,  0.0047, -0.0022]],\n\n         [[-0.0107, -0.0145, -0.0123,  ..., -0.0063, -0.0124, -0.0059],\n          [-0.0064, -0.0114, -0.0081,  ..., -0.0054, -0.0089, -0.0035]],\n\n         ...,\n\n         [[-0.0317,  0.0034, -0.0103,  ...,  0.0058,  0.0129, -0.0070],\n          [-0.0281,  0.0036, -0.0080,  ...,  0.0047,  0.0114, -0.0059]],\n\n         [[ 0.0179,  0.0233,  0.0038,  ..., -0.0051, -0.0023,  0.0128],\n          [ 0.0164,  0.0205,  0.0033,  ..., -0.0039, -0.0020,  0.0116]],\n\n         [[-0.0223,  0.0110, -0.0292,  ..., -0.0175,  0.1069, -0.0026],\n          [-0.0146,  0.0084, -0.0177,  ..., -0.0134,  0.0718, -0.0035]]]],\n       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 23\u001b[0m\n\u001b[1;32m     13\u001b[0m aattn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# contributions of tokens in original string\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# (B, H, L, D, L) @ (B, H, 1, L, E) => (B, H, L, D, E)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m+\u001b[39m safe_matmul(aa2, av2)\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(\"attn_out:\", attn_out.shape, attn_out)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(\"aattn_out:\", aattn_out.shape, aattn_out)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(attn_out, aattn_out[:\u001b[38;5;241m1\u001b[39m, :, \u001b[38;5;241m0\u001b[39m, :, :]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mattn_out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_out\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124maattn_out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maattn_out[:\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39m:,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;250m \u001b[39m:,\u001b[38;5;250m \u001b[39m:]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maattn_out[:\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39m:,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;250m \u001b[39m:,\u001b[38;5;250m \u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nattn_out: torch.Size([1, 16, 2, 64]),tensor([[[[ 0.0223,  0.0140,  0.0221,  ...,  0.0361, -0.0466,  0.0210],\n          [ 0.0123,  0.0040,  0.0118,  ...,  0.0154, -0.0209,  0.0109]],\n\n         [[-0.0042,  0.0049,  0.0140,  ..., -0.0007,  0.0082, -0.0055],\n          [-0.0033,  0.0045,  0.0096,  ..., -0.0007,  0.0047, -0.0022]],\n\n         [[-0.0107, -0.0145, -0.0123,  ..., -0.0063, -0.0124, -0.0059],\n          [-0.0064, -0.0114, -0.0081,  ..., -0.0054, -0.0089, -0.0035]],\n\n         ...,\n\n         [[-0.0317,  0.0034, -0.0103,  ...,  0.0058,  0.0129, -0.0070],\n          [-0.0280,  0.0036, -0.0079,  ...,  0.0047,  0.0114, -0.0060]],\n\n         [[ 0.0179,  0.0233,  0.0038,  ..., -0.0051, -0.0023,  0.0128],\n          [ 0.0164,  0.0205,  0.0033,  ..., -0.0039, -0.0020,  0.0116]],\n\n         [[-0.0223,  0.0110, -0.0292,  ..., -0.0175,  0.1069, -0.0026],\n          [-0.0146,  0.0084, -0.0177,  ..., -0.0134,  0.0718, -0.0036]]]],\n       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\naattn_out: torch.Size([1, 16, 2, 64]),tensor([[[[ 0.0223,  0.0140,  0.0221,  ...,  0.0361, -0.0466,  0.0210],\n          [ 0.0123,  0.0040,  0.0118,  ...,  0.0154, -0.0209,  0.0109]],\n\n         [[-0.0042,  0.0049,  0.0140,  ..., -0.0007,  0.0082, -0.0055],\n          [-0.0033,  0.0045,  0.0096,  ..., -0.0007,  0.0047, -0.0022]],\n\n         [[-0.0107, -0.0145, -0.0123,  ..., -0.0063, -0.0124, -0.0059],\n          [-0.0064, -0.0114, -0.0081,  ..., -0.0054, -0.0089, -0.0035]],\n\n         ...,\n\n         [[-0.0317,  0.0034, -0.0103,  ...,  0.0058,  0.0129, -0.0070],\n          [-0.0281,  0.0036, -0.0080,  ...,  0.0047,  0.0114, -0.0059]],\n\n         [[ 0.0179,  0.0233,  0.0038,  ..., -0.0051, -0.0023,  0.0128],\n          [ 0.0164,  0.0205,  0.0033,  ..., -0.0039, -0.0020,  0.0116]],\n\n         [[-0.0223,  0.0110, -0.0292,  ..., -0.0175,  0.1069, -0.0026],\n          [-0.0146,  0.0084, -0.0177,  ..., -0.0134,  0.0718, -0.0035]]]],\n       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
     ]
    }
   ],
   "source": [
    "# correct\n",
    "attn_out = torch.matmul(a, v)\n",
    "# testing\n",
    "def safe_matmul(a, b):\n",
    "    ashape = list(a.shape)\n",
    "    ap = a.reshape(math.prod(ashape[:-2]), ashape[-2], ashape[-1])\n",
    "    bshape = list(b.shape)\n",
    "    bp = b.reshape(math.prod(bshape[:-2]), bshape[-2], bshape[-1])\n",
    "    result = torch.matmul(ap, bp)\n",
    "    out_shape = ashape[:-2] + [ashape[-2]] + [bshape[-1]]\n",
    "    return result.reshape(*out_shape)\n",
    "\n",
    "aattn_out = (\n",
    "    # contributions of tokens in original string\n",
    "    # (B, H, L, D, L) @ (B, H, 1, L, E) => (B, H, L, D, E)\n",
    "    safe_matmul(aa1, torch.tile(av1, (1, 1, aa1.shape[2], 1, 1)))\n",
    "    # contributions of thought tokens generated so far\n",
    "    # (B, H, L, D, T) @ (B, H, L, T, E) => (B, H, L, D, E)\n",
    "    + safe_matmul(aa2, av2)\n",
    ")\n",
    "assert torch.allclose(attn_out, aattn_out[:1, :, 0, :, :]), f\"\\nattn_out: {attn_out.shape},{attn_out}\\naattn_out: {aattn_out[:1, :, 0, :, :].shape},{aattn_out[:1, :, 0, :, :]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "attn_out2 = layer.self_attn.o_proj(\n",
    "    attn_out.permute([0, 2, 1, 3]).reshape(b, l, model.embed_dim)\n",
    ")\n",
    "# testing\n",
    "aattn_out2 = model.bfloat_safe_apply(\n",
    "    layer.self_attn.o_proj,\n",
    "    aattn_out.permute([0, 2, 3, 1, 4]).reshape(ab, al, ad, model.embed_dim),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "x4 = residual + attn_out2\n",
    "# testing\n",
    "ax4 = aresidual + aattn_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "x5 = layer.post_attention_layernorm(x4)\n",
    "# testing\n",
    "ax5 = model.bfloat_safe_apply(layer.post_attention_layernorm, ax4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "x6 = layer.mlp(x5)\n",
    "# testing\n",
    "ax6 = model.bfloat_safe_apply(layer.mlp, ax5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct\n",
    "x7 = x4 + x6\n",
    "# testing\n",
    "ax7 = ax4 + ax6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
